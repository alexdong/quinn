Quinn Persistent Memory System Specification

Introduction

Quinn is a large language model (LLM) assistant that needs persistent memory to maintain context and personalization across conversations. LLMs are inherently stateless – they don’t recall information beyond their prompt window ￼. Without a long-term memory layer, an agent forgets user details and past interactions once a session ends ￼. This leads to repetitive questions and incoherent multi-session experiences. By contrast, a well-designed memory system lets Quinn remember user preferences, past solutions, and context over time, enabling more consistent and personalized assistance ￼ ￼.

This specification defines a MEMORY module for Quinn that captures both general user traits and domain-specific context. It outlines how memory is structured, stored, and retrieved, and how it updates incrementally as Quinn learns new information. The goal is to integrate a scalable, structured memory database – ideally with semantic (vector) search – so Quinn can recall relevant information when solving new problems or continuing a conversation ￼ ￼. The design emphasizes:
	•	Domain-independent memory: persistent traits like user tone, language preference, and personality.
	•	Domain-specific memory: context relevant to particular problem domains (e.g. preferred OS or tech stack for coding help, business project details).
	•	Incremental updates and milestones: continuous “write-as-you-learn” memory updates, plus explicit capture when the user asks to summarize or save key takeaways.
	•	Structured storage with vectors: a database format (e.g. JSON documents) augmented with vector embeddings for semantic recall.
	•	Agent integration: seamless retrieval of memory in Quinn’s email workflow, possibly via a retrieval layer or tool interface.

By following this spec, Quinn’s memory will persist across sessions, supporting a more coherent, adaptive, and personalized assistant behavior ￼ ￼.

Memory Categories and Scope

Quinn’s memory is organized into distinct categories to handle different scopes of information. This multi-level approach, inspired by state-of-the-art agent memory systems ￼, ensures relevant context is stored at the appropriate level:
	•	User Profile Memory (Domain-Independent): Persistent facts about the user that apply globally. This includes preferences for tone or formality, language, personal details, and general personality traits. For example, if a user prefers a friendly tone or likes responses in Spanish, that is stored in their profile memory. These traits are not tied to any single task domain and should be available in any context ￼. (In Letta AI, a “human memory block” stores user preferences and past interactions accessible across contexts ￼.) Quinn will maintain a per-user memory profile so it remembers each user’s style and important facts by default ￼ ￼.
	•	Domain-Specific Memory: Information tied to a particular domain, topic, or project. This captures context that is only relevant within that scope. For example, for coding help, Quinn might store the user’s preferred operating system (e.g. Linux) or tech stack (e.g. Python + Django). For business advice, Quinn might remember the company’s context or industry. Domain-specific memory can be segmented by tags or categories (like “coding”, “cooking”, “finance project X”), allowing Quinn to apply the right context only when that domain is in play. This is analogous to maintaining separate knowledge bases or sub-graphs per topic ￼ ￼. Quinn’s memory store should support group- or project-level memory to capture these contextual nuances ￼.
	•	Conversation/Episodic Memory: The running record or distilled summary of recent conversations (session-specific memory). During an ongoing conversation or email thread, Quinn uses short-term memory to stay coherent – typically by keeping recent messages or a summary of them in context ￼. This ephemeral memory resets or is archived after the session. However, important highlights from a session may be promoted into long-term memory (user or domain memory) via summarization. For instance, Quinn might keep a summary of a completed conversation about “Project X” in domain-specific memory for future reference, while discarding the full transcript. Many frameworks distinguish short-term vs. long-term memory, where short-term is limited to the current context window and long-term persists beyond it ￼ ￼.
	•	Global/Knowledge Memory: (Optional) A global memory accessible to all users or representing the assistant’s own accumulated knowledge. This could include the agent’s persona, common facts learned, or organizational knowledge not tied to one user. For example, Quinn’s persona memory (its own role, style, and expertise) might be stored as global memory ￼. Also, if Quinn integrates with a company knowledge base, some memory could be global business data. Global memory entries are available regardless of user, serving as a base layer of context (e.g. company policies or general world facts Quinn has “learned”). In practice, much of this overlaps with RAG (retrieval-augmented generation) knowledge stores, but any persistent facts Quinn learns during operation could be kept here.

Each memory entry will be labeled with its category/scope. This categorization helps ensure Quinn only retrieves relevant memories (e.g. pulling domain-specific memories only when in that domain’s context). Advanced systems like Zep implement multi-level memory graphs (user-level, group-level, session-level) to systematically manage these scopes ￼. Quinn’s memory should similarly distinguish per-user and per-domain knowledge versus ephemeral conversation history.

Memory Update Mechanisms (Incremental vs. Milestone)

Quinn’s memory system must support both continuous updates and explicit captures:
	•	Incremental Learning (Write-as-You-Learn): Quinn will continuously update memory during conversations by extracting important facts or preferences as they emerge. After each significant user turn or task result, the system should decide if new information merits storage. Best practices suggest using an LLM-based extractor to identify “salient information” to remember ￼. For example, if during a coding help session the user mentions “I switched from Windows to Linux last week,” Quinn’s memory module should capture this update (e.g. update the user’s preferred OS). Similarly, if the user reveals a preference (“I don’t like spicy food”), Quinn notes this in user profile memory. The Mem0 architecture demonstrates this approach: it dynamically extracts and consolidates salient info from ongoing conversations so only what matters gets stored ￼ ￼. By writing important tidbits immediately, Quinn avoids forgetting key details even if the conversation branches off or ends unexpectedly.
	•	Milestone-based Capture: At certain points, Quinn will perform a deliberate summarization or snapshot of context – either triggered by the user or by the system’s policy. A user may explicitly say “Quinn, summarize this discussion and remember it for later”, indicating a milestone to store. Quinn should then produce a concise summary or takeaway list and save it as a memory entry (tagged accordingly, e.g. as a summary of that project or decision). Milestones also occur implicitly when conversations grow long. To avoid context overflow, Quinn can summarize older messages once a threshold is reached, preserving important facts while discarding detail ￼ ￼. For instance, if an email thread exceeds a token limit, Quinn might compress earlier emails into an “archive summary” and store that in long-term memory, effectively a milestone capture. This technique is used in practice: Letta automatically creates a recursive summary when the context window is full ￼, and LangChain-based memory policies can trigger summarization once a token count is exceeded ￼. Such triggered summarization ensures the conversation’s essence is remembered without exceeding limits.
	•	User-Initiated Memory Actions: Quinn’s interface can allow users to directly influence memory. Phrases like “Remember that I prefer XYZ” or “Don’t forget this detail” should prompt Quinn to acknowledge and store that information. The agent should confirm the memory (“Noted, I’ll remember that you prefer XYZ”) and write it to the appropriate memory section (profile or domain). This explicit signal is high-value because it clearly identifies what the user considers worth remembering. The system should have a straightforward command or API call for this (e.g., a save_memory function the agent can invoke when it detects such a request).
	•	Automated Reflection: Optionally, Quinn can periodically perform a “reflection” step outside of active conversations to consolidate learning. For example, after a resolved task or at the end of day, Quinn might review recent interactions and distill any new enduring facts. This is similar to milestone summarization but on a schedule or trigger (like end of a support ticket). It provides an opportunity to clean up memory (merging duplicate info, expiring irrelevant items) and ensure important knowledge is captured in a structured form.

These mechanisms ensure memory is both up-to-date and not overly cluttered. Incremental updates capture fine-grained details in the moment, while milestone summaries provide higher-level context snapshots. Quinn’s memory logic will apply filters and decay to avoid storing trivial or stale data ￼. For instance, if a fact is later contradicted or becomes irrelevant, the system might mark the old memory as expired or reduce its importance. By balancing continuous learning with periodic summarization, Quinn’s memory remains relevant, concise, and efficient.

Memory Structure and Data Format

All persistent memories are stored as structured data records to facilitate organization, querying, and updates. We propose using a JSON-based schema for memory entries, which can also be expressed in YAML for readability if needed. JSON is well-suited because it’s machine-readable and easily stored in NoSQL or document databases, and it can encode nested metadata. A defined JSON Schema can enforce data types and required fields for each memory entry. Key fields in the memory schema include:
	•	id (string/UUID): Unique identifier for the memory record.
	•	user_id (string): Identifier for the user this memory is associated with. (For global or agent-level memories not tied to a single user, this could be null or a special value.) ￼
	•	domain (string): The domain or category tag, if applicable (e.g. "coding", "travel", "global"). This helps scope the memory’s relevance.
	•	type (string): The type of memory or its role. For example: "preference", "fact", "summary", "persona", etc. This is a high-level tag indicating what kind of knowledge it is.
	•	content (string or object): The information to remember. For textual facts or summaries, this is a string. It could also be structured (for example, a JSON object of key-value pairs for user settings). This field stores the core memory data (e.g., "John is a premium customer with account ID 12345." or {"preferred_os": "Linux", "tech_stack": "Python/Django"}).
	•	metadata (object): Additional metadata about the memory. This can include subfields like source (where or when it was learned, e.g. “email thread May 2025”), importance_score (a numeric weight indicating how important or relevant this memory is), context (e.g. conversation/session ID if from a specific interaction), and any custom tags. Mem0, for instance, allows custom metadata and tagging on memories for filtering ￼.
	•	tags (array of strings): A list of tags or categories for quick classification. Tags might mirror the domain and type but can be more granular. For example: ["preference", "tone"] or ["projectX", "summary"]. These enable fast filtering (e.g. find all memories tagged “projectX”).
	•	created_at (datetime): Timestamp when this memory was first created (learned).
	•	updated_at (datetime): Timestamp of the last update to this memory (if it has been modified or re-validated). For immutable entries that never change, this can be the same as created_at ￼.
	•	version (integer or string): Version identifier for the memory’s content. Every time the content is significantly modified (e.g. user changed a preference), the version increments, or a new record is created with a higher version. This helps track evolution of information (see Versioning below).
	•	expiration_date (datetime, optional): If this memory is meant to decay or expire (e.g. ephemeral info), this field indicates when it can be purged. Permanent memories have this null. (Mem0 uses a similar concept of expiration for memories that should eventually be forgotten ￼.)
	•	immutable (boolean, optional): Flag indicating if this memory should never be altered or deleted by the agent once written ￼. For example, a factual memory like a birthday might be immutable, whereas a “current mood” memory could be mutable.

Encoding Format: We suggest using JSON for storage and interchange, with a defined JSON Schema to validate memory objects. For ease of manual editing or config files, a YAML representation could be used interchangeably (since YAML can be mapped to the same structure). For instance, an example memory entry in YAML might look like:

id: "mem-001234"
user_id: "user-123"
domain: "coding"
type: "preference"
content: "Preferred OS: Linux"
tags: ["preference", "OS", "coding"]
metadata:
  source: "conversation_2025-07-14"
  importance_score: 0.8
created_at: "2025-07-14T12:45:00Z"
updated_at: "2025-07-14T12:45:00Z"
version: 1

And a user profile memory example in JSON:

{
  "id": "mem-001235",
  "user_id": "user-123",
  "domain": "global",
  "type": "preference",
  "content": {"tone": "formal", "language": "Spanish"},
  "tags": ["profile", "tone", "language"],
  "metadata": {
    "source": "user_profile_update",
    "notes": "Set via onboarding survey"
  },
  "created_at": "2025-07-01T09:00:00Z",
  "updated_at": "2025-07-01T09:00:00Z",
  "version": 1
}

This structure is flexible enough to store both simple textual memories and structured data. It is crucial that the memory store can be queried by these fields (e.g. find all content or metadata matching certain keywords, filter by user_id or tags). The schema can evolve with a versioning strategy (for the schema itself) – e.g. MEMORY schema v1.0 might later add new fields like confidence or adjust formats. Including a top-level schema_version in each record or in the database configuration can help manage format changes over time.

Using a structured format with explicit fields ensures Quinn’s memories are organized and machine-interpretable, not just raw text blobs. This lays the groundwork for efficient filtering, retrieval, and updates via code.

Tagging and Metadata

Tagging: Each memory entry can carry one or multiple tags that describe its content or context. Tags provide a lightweight way to classify and retrieve memories. We recommend establishing a tagging taxonomy for Quinn’s memory: for example, tags like #preference, #personality, #tech, #projectX, #summary, etc. Domain labels can be tags as well (e.g. #coding, #cooking). Tagging serves several purposes:
	•	It enables filtering and conditional retrieval. Quinn’s retrieval function can request “all memories tagged with #projectX” or exclude memories tagged #persona when assembling domain-specific context. Mem0’s memory system supports filtering queries by metadata and categories (tags) using logical conditions ￼ ￼, which Quinn’s should emulate. For instance, Quinn could fetch memories where user_id = X AND tags contains 'finance' to get only finance-related facts for that user.
	•	It facilitates organization. By tagging memories, we can easily maintain separate clusters of knowledge. E.g., tagging something as #global vs #user_specific vs #domain:XYZ. This helps avoid mixing irrelevant contexts.
	•	It aids maintenance. We can run jobs to clean or review memories by tag (e.g., purge all #temporary tagged items past their expiration, or review all #feedback entries from the user for learning improvements).

Quinn’s memory model should allow multiple tags per entry for flexibility. These tags should be stored as part of the memory record (e.g., in a list field as shown in the schema above). Many existing memory frameworks include categories or tags; for example, Mem0 attaches a categories list to each memory object ￼ and Letta’s core memory blocks can be custom-labeled ￼. We will follow similar practice.

Metadata: In addition to tags, each memory has a metadata field for key-value annotations. This is more expressive and can capture arbitrary detail about the memory. Some useful metadata keys and their usage:
	•	Source/Origin: e.g. source: "email_thread_2025-07-14" or ingested_from: "ProjectBrief.docx". Knowing where a memory came from can help in validating it or providing context. The agent might even surface source if needed (“you mentioned this in an email on July 14”).
	•	Importance: an internal score (perhaps 0.0 to 1.0) indicating how critical this memory is. Quinn might assign a higher importance to something the user explicitly said to remember, or to core profile info like name. This score could be used to prioritize which memories to include when space is limited (only include top N important memories) or to decide which ones not to expire. Some frameworks implement memory decay by reducing importance over time for seldom-used info ￼.
	•	Confidence: if the memory was inferred or extracted by the LLM (rather than stated by user), we might include a confidence level. E.g., if Quinn summarized a long text into a fact, confidence might reflect how certain the summary is correct. This can guard against storing hallucinations.
	•	Relations: pointers to related memory entries (could be refs to other ids). For instance, if one memory is “User’s laptop OS is Linux” and another is “User’s preferred IDE is VSCode”, they might both have a relation to a higher-level concept “User’s development environment”. A knowledge graph approach like Zep’s would explicitly represent these relationships ￼, but in a simpler system, we can store relation IDs in metadata to link memories.
	•	Validity/Temporal Info: if a memory has time-bound validity, metadata can store that (or we use dedicated fields like expiration_date). For example, valid_from: 2025-07-01, valid_until: 2025-07-31 if a fact was only true in July. This ties into versioning and is important for tracking changes over time (see below on temporal versioning) ￼.
	•	Immutable/Lock: If an entry is not meant to change, a metadata flag or the immutable field can note that ￼. Quinn’s logic would then refrain from editing this memory – if an update is needed, a new entry should be created instead of mutating.

Both tags and metadata provide a rich context that pure language content doesn’t. They allow the memory database to be queried in structured ways (e.g., find by date range, by user, by importance threshold, by specific keys, etc.). The memory service should expose APIs to query by these fields (for instance, Mem0’s API supports advanced filters on metadata and categories for retrieval ￼). In practice, Quinn can use this to do things like: “search memories where user_id = U123 and domain = coding and contains ‘database’” if the user asks a database question – ensuring relevant coding-related memories for that user are fetched.

Versioning and Temporal Memory

Over time, information about the user or context may change. The memory system needs a strategy to handle updates without losing historical data. Key principles for versioning:
	•	No Blind Overwrites: Avoid simply mutating a memory entry in place when something changes, as that erases history (a limitation noted in basic memory stores) ￼. Instead, implement versioned records or temporal tagging. For example, if the user’s preferred editor was “VSCode” but later they say they now use “PyCharm,” the system should either:
a) Create a new memory entry “Preferred editor: PyCharm” with a new ID, marking the old one as outdated, or
b) Update the existing entry’s content to “Preferred editor: PyCharm” but increment its version and archive the previous value.
	•	Temporal Validity: One robust approach is to store validity periods with each memory fact ￼. The old preference “VSCode” could get a field valid_until: 2025-09-01 (the date of change), and the new preference gets valid_from: 2025-09-01. This bi-temporal data model means even if preferences evolve, we retain the timeline. Zep’s memory system uses bi-temporal facts to track how knowledge changes, rather than deleting or overwriting ￼. Adopting this, Quinn’s memory entries can have optional valid_from and valid_to metadata. When retrieving, the system by default uses only currently valid facts (e.g., now < valid_to or valid_to null). If needed, historical info can still be queried.
	•	Version Field: As noted, each entry can have a version number or string. If simply appending new records for changes, the version can indicate sequence (e.g., “v2” for the updated fact, with a pointer to the previous version’s ID). If modifying in place, version still increments to indicate a change (and the old content might be saved in a separate history log). Having a version helps with debugging and ensuring consistency (we can quickly see if a memory was updated and how many times).
	•	Soft Deletes and Archiving: Instead of hard-deleting memory entries, we use flags or timestamps for deactivation. For example, when a piece of info is no longer true, mark it as inactive (maybe an active: false flag or set expiration_date). This way, if needed for audit or retrospective, we still have the record. The memory retrieval logic will exclude inactive memories unless explicitly asked. Mem0’s comparison highlights that lacking versioning (overwriting in-place) is a downside ￼; we avoid that by never permanently destroying a memory unless policy demands.
	•	Merging and Consolidation: In some cases, multiple memories might be consolidated into one for efficiency. For instance, if Quinn has separate memories “User is vegetarian” and “User is dairy-free” and later combines them into “User dietary preference: vegetarian and dairy-free,” the system should retain the original pieces at least in history, but mark them superseded by the merged entry. We can use metadata like superseded_by: <new_id> on old ones, and perhaps increase their version or set an expiration. The new merged memory gets a fresh version or id. This is part of memory cleanup and ensures we don’t have conflicting or redundant facts floating around.
	•	Schema Versioning: Separately from data versioning, we maintain a schema version for the memory format itself. For example, “MEMORY.md Spec Version 1.0”. If later we change the structure (add fields like confidence or adjust how domain is represented), we bump the schema version. Memory records could optionally store which schema version they conform to (to handle backward compatibility if needed). In most cases, though, evolving the system in a controlled way and migrating old records is sufficient.

By implementing version control in memory, Quinn can handle evolving user data gracefully. For example, if a user’s name changes (perhaps they got married and have a new last name), Quinn can update the name in memory but still recall the previous name if context requires (“You might know me as Alice Smith, I recently changed to Alice Johnson”). Or if a project detail is revised, earlier assumptions aren’t lost. This history also aids debugging: developers or the system can inspect how a conclusion was reached based on what Quinn “knew” at that time.

In summary, persistent memory with history prevents regressions in user experience and maintains trust. As the Zep vs Mem0 comparison shows, having proper versioning and temporal context is a modern best practice ￼ ￼. Quinn’s memory will treat knowledge as an ever-growing journal rather than an overwriteable scratchpad.

Storage and Retrieval Layer

To store the memory records defined above and support efficient recall, we recommend a hybrid database approach: a structured data store combined with a vector search index.

Structured Storage: Each memory entry (JSON document) can be stored in a document-oriented database or relational DB with JSON support. A document database (like MongoDB, Couchbase, or PostgreSQL JSONB) can naturally store the hierarchical JSON. The store should allow indexing on key fields like user_id and tags for filtering. Alternatively, a relational model could be used (with tables for users, memory entries, etc.), but given the flexibility needed (especially with varying metadata), a document store is convenient. If using a relational DB, one can still store the content or metadata as JSON and have columns for the main fields (id, user_id, type, etc.). The key requirement is the ability to query by fields and to retrieve or update specific records quickly.

Vector Database for Semantic Recall: For semantic similarity search, a vector index is crucial. This allows Quinn to fetch memories not just by exact keyword, but by meaning. The idea is to embed each memory’s content (and perhaps some metadata) into a high-dimensional vector using an embedding model, and store these vectors. When a new query or problem arrives, Quinn’s system generates an embedding for the query and performs a nearest-neighbor search in the vector space to find relevant past memories. This helps retrieve things like “This sounds similar to that issue we solved last month” even if no keywords match exactly. There are several suitable vector database options (both open-source and managed) – Mem0’s framework, for example, supports integration with Qdrant, Chroma, Milvus, PostgreSQL (pgvector extension), Redis (vector), etc ￼. For Quinn, a good choice might be an open-source vector DB like Chroma or Qdrant for local deployment, or a managed service like Pinecone or Weaviate for scalability. Even PostgreSQL with pgvector could suffice if using a smaller scale and wanting simplicity (it allows vectors alongside relational data).

The memory system can marry these two layers: store the JSON records in a database and store the embeddings in a vector index that references the record IDs. Many vector DBs natively support storing a metadata payload with each vector (for instance, Pinecone, Qdrant, Weaviate all allow attaching JSON metadata to vectors). This means we could potentially use a single system – e.g., Qdrant – to store both the vector and the memory content in metadata. Queries can then filter by metadata (user, tags) and do similarity search on the vectors simultaneously, which is very powerful. Mem0 leverages such capabilities: it can filter memory search by user/session and metadata, and then perform semantic ranking ￼ ￼. Quinn’s retrieval queries will often include a filter for the active user and domain, so the vector search happens only on relevant subset (for example, “search within user=123 and domain=coding memories for relevance to query embedding”).

Database Recommendations:
	•	If simplicity and open-source are priorities, Chroma DB (an open-source vector store with persistent storage) could be used to store memory embeddings with metadata. Alternatively, Qdrant (vector DB) with an attached collection per user or global collection with filtering is a solid option ￼. Both have Python clients for integration.
	•	If using a relational DB like Postgres, the pgvector extension can be enabled to store embeddings in a column (vector type) and perform similarity search via SQL. This way, everything (structured fields and vector) lives in one place (e.g., a memories table with columns: id, user, domain, content_json, embedding). This is convenient for moderate scale and consistency.
	•	For enterprise scale or managed solutions, Pinecone or Weaviate offer robust vector search as a service. They can handle large volumes and provide filters. If Quinn’s deployment expects lots of memory entries and frequent searches, a managed vector DB might simplify ops. Zep, for instance, provides a cloud service that effectively is a specialized memory store with vector and graph capabilities ￼.
	•	Optionally, a separate key-value store (like Redis) can be used for super quick lookups of known keys (for example, storing the entire user profile JSON under a key user:123:profile). This is useful for grabbing frequently needed memory (like user name, preferences) without a full search. Redis even has a vector similarity module now (Redis Vector Similarity) ￼, which could double as the vector store if chosen.

Indexing and Retrieval: All memory records should be indexed by user_id and other common filters to speed up queries. The vector index covers semantic retrieval, but sometimes Quinn will retrieve by exact keys – e.g., fetch the user’s profile or fetch all facts tagged #projectX. The system should support queries like: get_all(user_id=X, domain=“finance”) which returns matching memory entries without semantic ranking (useful for assembling context) ￼ ￼. Indeed, Mem0 offers a get_all to retrieve memories by attribute filters without a semantic query ￼; Quinn’s memory API should do the same for bulk retrieval when needed. For semantic queries, the search function will take an embedding (from the new prompt or problem description) and return the top N similar memory entries, along with their content and metadata ￼ ￼. We will include the score of similarity if needed to decide cutoff ￼.

It’s wise to incorporate hybrid retrieval strategies: e.g., combine semantic search with keyword matching and even graph-based search for precision ￼. For example, if the user asks a very specific question (“What is my AWS access key?”), a pure embedding search might miss it if it’s stored exactly. In such cases, enabling an exact keyword filter or query is useful. Mem0 provides a keyword_search toggle to include exact matches alongside embeddings ￼. Quinn’s system can similarly try a keyword lookup on content or metadata (like find memory with content containing a rare term). This improves recall. For most cases, though, semantic search is primary since user queries may not exactly match how memories were phrased.

Finally, given Quinn’s memory will grow, we must consider scalability. The storage engine should handle possibly thousands of memory entries per user (depending on usage) and still retrieve in low latency (<100ms ideally). Vector search can be optimized by limiting vector dimensions (using a suitable embedding model) and using approximate nearest neighbor indexing. The structured DB should be indexed on fields used in queries to avoid full scans. Partitioning memory by user or domain could also help (e.g., separate collections or index partitions per user).

In summary, a structured JSON store + vector index approach balances flexibility and power: Quinn can retrieve memories by structured filters (user, tags, recency) and by semantic similarity. This ensures that when a new problem arrives, Quinn can automatically pull up relevant past knowledge (via vector recall) and also always respect user-specific scope (via metadata filtering) ￼ ￼. The result is a memory system that is both content-aware and context-aware, stored durably for long-term use.

Integration with the LLM Agent (Quinn)

The persistent memory will be tightly integrated into Quinn’s workflow, which primarily involves email communications and possibly tool use. Here’s how memory integration is designed:
	•	Context Injection: Whenever Quinn generates a response (e.g., drafting an email reply), it should have access to relevant memory entries to include in its prompt context. This can be done by pre-pending a summary of relevant memories to the conversation history or as additional system instructions. For example, if Quinn is about to answer a coding question via email, the system might retrieve the top 3 coding-related memories for that user (e.g., “User prefers Python on Linux”, “User struggled with Docker last time”, “User uses AWS for deployment”) and include them as a brief context paragraph or as part of the system prompt. This gives the LLM the needed background. The process could be: On receiving a user email, generate an embedding of the email content, query the memory DB with filters (user_id plus relevant domain tags) for similar past issues or stated preferences, and retrieve those memory snippets to feed into the LLM. This is akin to retrieval-augmented generation except using the agent’s own conversation memory instead of a generic knowledge base ￼ ￼.
	•	Tool/Function Calls for Memory: If Quinn is implemented as an LLM with function-calling abilities, memory retrieval can be exposed as a tool. For instance, a function search_memory(query, filters) could be registered. The LLM (Quinn) could decide to call this function when needed to fetch information. The system might sometimes explicitly prompt Quinn to do so by providing a system message like “If you need to recall something, you can use the search_memory function.” Upon call, the function would perform the vector/metadata query and return the results, which the LLM can incorporate into its answer. This agent-tool interaction approach is powerful for long dialogues: Quinn can dynamically query its memory mid-conversation (say the user asks “You remember the budget we discussed last month?”, Quinn can call memory search with a hint for “budget last month” and retrieve that info to answer).
	•	Email Thread Continuity: Since Quinn works over email, each email thread can be treated akin to a chat session. Quinn should thread messages together and maintain a thread-specific short-term memory (like a conversation buffer) for the immediate context. In addition, it should reference the long-term memory for any background. If a new email comes in referencing an older conversation (“Regarding our last discussion on Project X…”), Quinn should fetch the summary or key points of that Project X discussion from memory to refresh itself before answering. This prevents Quinn from asking the user to repeat details that were already shared in previous emails ￼ ￼. Essentially, Quinn uses memory to achieve persistence across emails, behaving as if it truly “remembers” prior threads with that user.
	•	Retrieval Layer Interaction: If an external retrieval system or knowledge base is present (for example, a company wiki or documentation), Quinn’s memory system can interface with it as well. There might be a combined approach: first query Quinn’s internal memory; if insufficient, query external sources. The design should allow Quinn to pass queries to both its long-term memory and any domain-specific database. Memory entries could even store pointers into external data (e.g. memory content: “See KnowledgeBase article 42 for more details on X”).
	•	Memory Updates During Conversations: As Quinn drafts a reply or finishes handling an email, it should update memory as needed. For example, if the user provided new info (“The server is now running Ubuntu 22.”), Quinn’s reply will acknowledge it and in the background the system should update the user’s environment memory (perhaps via a background process or another function call like update_memory(key="server_os", value="Ubuntu 22")). If the user explicitly says “please remember X”, Quinn’s flow should include a step to save that as described. Some agent frameworks allow callbacks or middleware after generating a response – that can be used to trigger memory writes. Alternatively, Quinn can be instructed to output a specially formatted section (like hidden in the email or via a function call result) that the system intercepts to know what to save. The exact implementation can vary, but the spec is that no important information that arises should be lost.
	•	Email Formatting Consideration: Because Quinn communicates via email, lengthy context may not be directly visible to the user (we can supply context to the model that isn’t sent as part of the email text). That means memory retrieval results would be used internally to guide the response generation. For traceability, Quinn might include subtle cues in its email like “As we discussed previously, …” to leverage memory (if appropriate), but it won’t dump raw memory contents to the user unless it makes sense. The memory spec mainly concerns internal use, but transparency can be managed (Quinn could cite the source of memory if the user needs, like “(from our conversation on June 5th)”).
	•	Handling Conflicts or Stale Info: The agent integration layer should also handle cases where memory might conflict with current input. If the user says something now that contradicts a stored memory, the fresh input likely takes precedence. Quinn’s prompt assembly logic might de-emphasize or omit conflicting memory entries (or even mark them for update). For example, memory says “User’s preferred language = Python”, but the user’s latest email says “I want to try Java this time.” In such a case, Quinn should not push the Python preference context, and should update that preference after. Some reasoning could be built in: if a memory is older and the user now states a new preference, consider the new info as truth and schedule the old memory to be versioned out.
	•	Security and Privacy: Since memory stores potentially sensitive data (preferences, possibly account info, etc.), integration must ensure only authorized access. If Quinn serves multiple users, each user’s memory is isolated (filter by user_id always). The memory database should be secure and likely encrypted. Also, Quinn’s outputs should be careful not to leak another user’s info. These are general considerations outside the scope of pure design, but worth noting: per-user memory isolation is a must.

Overall, integration means whenever Quinn is deciding what to say or do, it consults its memory to ground the response in past knowledge. This turns a stateless LLM into a stateful agent that can build on earlier interactions ￼ ￼. The result: Quinn will not ask the same question twice, will remember to follow the user’s style preferences, and will reuse relevant details from previous emails to provide continuity. Technically, this is achieved via a retrieval step in the agent pipeline (which can be automated or via explicit function calls). The memory retrieval and injection becomes part of Quinn’s reasoning cycle for every message.

Examples of Memory Usage

To illustrate how Quinn’s persistent memory works, here are a few scenarios showing what gets remembered and when:
	•	Remembering User Preferences (Domain-Independent): Suppose in an initial setup, the user tells Quinn, “I prefer if you communicate in a formal tone, and please respond in Spanish.” Quinn will store this in the user profile memory (e.g., tone: formal, language: Spanish). Now, in all subsequent communications, Quinn recalls these settings. When drafting an email, it might think (internally) “Use formal phrasing and Spanish language as per memory.” If later the user says “Actually, you can use a casual tone with me,” Quinn updates the tone preference memory (versioning the old preference) and from then on writes in a casual style. This domain-independent preference is always retrieved for any conversation – it’s part of Quinn’s core memory of the user.
	•	Domain-Specific Context Recall: The user often asks coding questions. In one session, the user said, “My project is in Python running on a Linux server.” Quinn’s memory module extracts that fact and stores it tagged under coding domain (e.g. memory content: “User’s project environment: Python on Linux”). A week later, the user emails Quinn: “The deployment isn’t working, can you help debug?” Quinn’s system sees this is a coding/domain query, so it retrieves domain-specific memories. The memory about the Python/Linux environment is fetched (via semantic match on “deployment” and by filter domain=coding). Armed with that, Quinn can immediately tailor its answer to that stack: “Given you’re using Python on Linux, one thing to check is the file permission issues on deployment.” The user doesn’t have to reiterate their tech stack – Quinn remembered it. If at some point the user switches the project to a different tech (say Node.js on Docker), Quinn would record that as a new memory and likely mark the old Python/Linux memory with an end date. Future answers then align with the new stack.
	•	Episodic Memory and Summarization: During a long support interaction via email, many messages are exchanged troubleshooting an issue. Quinn cannot include the entire thread in context once it grows beyond the model’s token limit. Instead, after the thread passes, say, 50 messages, Quinn triggers a summarization milestone ￼. It generates a summary like, “Summary of Issue #1234: We diagnosed an API timeout, user applied a fix by increasing memory, issue resolved.” This summary is stored as a memory entry tagged #summary and perhaps linked to that support ticket ID. Older emails might then be dropped from immediate context, but the summary remains. One month later, if the user references that issue (“This is similar to issue #1234 we had”), Quinn can quickly retrieve the stored summary and recall the key points without reading all old emails. This improves efficiency and coherence across episodes.
	•	User-Requested Remembering: A user might explicitly instruct, “Quinn, remember that my birthday is October 10.” Quinn will comply by creating a memory (perhaps type: fact or personal_info) with content “User’s birthday is Oct 10”. It might tag it as #personal. Later, if the user casually asks, “Do you know what special date is coming up for me?”, Quinn can search its memories for anything tagged personal or containing date info, find the birthday fact, and respond appropriately. This example shows Quinn capturing a fact at the user’s request (milestone capture) and using it much later in a relevant context. Similarly, if the user says “remember this account number for next time” in a finance query, Quinn stores it securely and recalls it on the next related query (assuming security policy allows).
	•	Adapting to Feedback and Avoiding Repetition: Suppose in a conversation the user got annoyed by too much detail in answers. They say, “You’re giving me too much information. Please be more concise.” This feedback is important to Quinn’s interaction style. Quinn can record a memory like {"type": "preference", "content": "prefers concise answers", "domain": "global"}. The next time Quinn generates an answer, it checks memory and finds this preference. It then adjusts its output length accordingly. Also, because this is stored, Quinn won’t revert to verbose answers in future sessions – it consistently remembers to keep things concise for this user. Many commercial systems lack this and repeat mistakes, but Quinn’s persistent memory prevents that by learning from feedback across sessions ￼ ￼.
	•	Cross-Session Continuity (Personalization Example): In one email session, the user mentioned their dog named Rex and that they were busy because of a dog training class. Quinn might note that in a general user memory: “User has a dog named Rex.” Later, in an unrelated conversation weeks after, the user casually says, “I finally have some free time now.” Quinn, remembering the previous context, might respond: “Glad to hear that! How’s Rex doing?” — a personalized touch that comes from long-term memory. This kind of personal continuity can delight users as it shows the agent “remembers” their life details (as long as it’s done appropriately). It turns the AI from a stateless Q&A machine into something more like a relationship or at least a consistent persona ￼ ￼. (Of course, Quinn should use such personal memories judiciously and only when contextually relevant.)
	•	Remembering Outcomes to Avoid Redundant Solutions: Quinn helped the user in the past with a configuration issue and the solution was non-trivial. Quinn stored the outcome: “Resolved config error by changing X to Y in settings.” Now, months later a similar problem arises. Quinn’s memory search finds that past solution (due to semantic similarity in problem description). Quinn can then say, “This issue resembles what we encountered before; last time, changing X to Y solved it ￼. Let’s verify if that applies here.” This saves time and shows expertise. In contrast, without memory, Quinn might have either forgotten and troubleshooting from scratch or recommended something the user already tried. With memory, it builds on past solutions and avoids repeating the same steps, thereby improving efficiency.
	•	Versioning in Action (Temporal Memory Example): Initially, Quinn has a memory “Project Alpha deadline is June 30, 2025.” Later, the user informs an update: deadline moved to July 15, 2025. Quinn updates this info by creating a new memory entry or version. The old memory might be marked with valid_until: 2025-06-30 and new one valid_from: 2025-06-30. When the user asks “What’s the deadline for Project Alpha?”, Quinn only surfaces the current one (July 15). If needed, it could mention the change if relevant, but likely it will just answer with the updated info. The history of change is preserved internally. If the user or an admin audits the memory, they’d see the earlier entry and the updated one, with timestamps and versions. This is important for trust – if Quinn were to ever give an answer based on outdated info, one could trace why. But by default, versioning ensures Quinn references the newest facts.

Each of these examples demonstrates Quinn’s memory capturing something at the right moment (either incrementally as facts emerge, or when explicitly prompted), and then paying it forward in future interactions. By structuring the memory as described and leveraging it during email conversations, Quinn will exhibit behavior that feels consistent and attentive.

For instance, in a real-case snippet: a user named John with a premium account contacts Quinn support. Initially, John says “Hi, I’m John and I need help with my premium account.” Quinn stores John’s name and account type. During that conversation, Quinn uses it: “Hi John! I see you have a premium account… how can I help?” (short-term use) ￼. A month later, John emails again. Quinn’s greeting: “Welcome back, John! How’s your premium account going? Last time we discussed your billing issues.” Here Quinn seamlessly pulled up John’s identity and the topic from memory, making the interaction feel human-like and continuous ￼. Without persistent memory, Quinn could not do this; with the specified memory system, it’s straightforward.

In summary, Quinn’s persistent memory will remember both the personal touches (like a user’s language, name, past chats) and the technical context (past problems, solutions, preferences in each domain). It updates this knowledge base on the fly and on command. By storing data in a structured, queryable form with semantic search, Quinn can match new questions to old answers, maintain context across long periods, and truly “learn” from each interaction. This specification ensures that Quinn evolves from a stateless LLM into a stateful assistant with a growing memory, leading to more efficient support and a better user experience ￼ ￼.
